{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Signals Analysis - Week 8 - Image Processing Part 1\n",
    "### Table of Contents:\n",
    "- [The Evolution of Imaging: From Light to Digital Representation](#history)\n",
    "- [Digital Images: Mathematical Foundations](#digital)\n",
    "- [Basic Image Operations and Transformations](#transformations)\n",
    "- [Image Enhancement and Restoration](#restoration)\n",
    "- [Edge Detection and Feature Extraction: Mathematical Foundations](#edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Evolution of Imaging: From Light to Digital Representation <a id='history'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Physics of Light and Image Formation\n",
    "\n",
    "To understand imaging, we must first understand light itself. Light is a form of electromagnetic radiation that exhibits a fascinating dual nature: it behaves as both a wave and a particle. This duality, a cornerstone of modern physics, helps us understand how images form and how we can capture them.\n",
    "\n",
    "As a wave, light is characterized by its wavelength (λ) and frequency (ν), which are related through the speed of light (c) by the equation:\n",
    "\n",
    "$c = λν$\n",
    "\n",
    "Let's break this down: if you think of light as a wave on water, the wavelength is the distance between two consecutive wave peaks, while the frequency is how many waves pass a fixed point each second. The speed of light (approximately 3 × 10⁸ meters per second) remains constant in a vacuum, so as wavelength increases, frequency must decrease, and vice versa.\n",
    "\n",
    "The visible spectrum—the light our eyes can see—occupies only a small portion of all possible wavelengths, ranging from about 380 nanometers (violet) to 700 nanometers (red). A nanometer is incredibly small: one billionth of a meter. When light acts as particles, we call these particles photons. Each photon carries a specific amount of energy (E), determined by its wavelength:\n",
    "\n",
    "$E = \\frac{hc}{λ}$\n",
    "\n",
    "Here, h is Planck's constant (approximately 6.626 × 10⁻³⁴ joule-seconds), a fundamental constant of nature. This equation tells us something important: shorter wavelengths (like blue light) carry more energy than longer wavelengths (like red light).\n",
    "\n",
    "When light interacts with matter, several key phenomena occur, each governed by specific physical laws:\n",
    "\n",
    "1. Reflection is perhaps the most familiar—when light bounces off a surface. The law of reflection states that the angle at which light hits a surface (angle of incidence, θᵢ) equals the angle at which it bounces off (angle of reflection, θᵣ):\n",
    "\n",
    "$θ_i = θ_r$\n",
    "\n",
    "2. Refraction occurs when light passes from one medium to another, causing it to bend. This bending is described by Snell's law:\n",
    "\n",
    "$\\frac{n_1}{n_2} = \\frac{\\sin(θ_2)}{\\sin(θ_1)}$\n",
    "\n",
    "where n₁ and n₂ are the refractive indices of the two materials (essentially a measure of how much they slow down light), and θ₁ and θ₂ are the angles before and after the light bends. This is why a straw in a glass of water appears to be bent—light bends as it moves between air and water.\n",
    "\n",
    "3. Absorption occurs when materials take in light energy. The Beer-Lambert law describes how light intensity decreases as it passes through a material:\n",
    "\n",
    "$I(x) = I_0e^{-αx}$\n",
    "\n",
    "Here, I₀ is the initial light intensity, x is the distance traveled through the material, and α is the absorption coefficient (how strongly the material absorbs light). The exponential nature of this equation means that absorption happens most rapidly at first and then slows down.\n",
    "\n",
    "4. Diffraction occurs when light waves bend around obstacles or through openings. For a diffraction grating (a surface with many parallel slits), the relationship is:\n",
    "\n",
    "$\\sin(θ) = \\frac{mλ}{d}$\n",
    "\n",
    "where θ is the angle of diffraction, m is an integer representing the order of diffraction, and d is the distance between slits. Diffraction explains why we can see interference patterns and why there are fundamental limits to image resolution.\n",
    "\n",
    "## The Historical Journey of Imaging\n",
    "\n",
    "### Early Beginnings: The Camera Obscura\n",
    "\n",
    "The story of imaging begins with a simple observation: light passing through a small hole into a dark room creates an inverted image of the outside world. This phenomenon, known as the camera obscura (Latin for \"dark chamber\"), was first described in ancient China by the philosopher Mo Di around 400 BCE. However, it wasn't until the 11th century that Ibn al-Haytham provided a comprehensive mathematical treatment of this effect.\n",
    "\n",
    "The mathematics of the camera obscura is elegantly simple. If we have an object of height h₀ at distance d₀ from the pinhole, it creates an image of height hᵢ at distance dᵢ behind the pinhole:\n",
    "\n",
    "$h_i = \\frac{h_o}{d_o}d_i$\n",
    "\n",
    "This equation reveals the inherent trade-off in pinhole imaging: moving the screen farther back (increasing dᵢ) makes the image larger but dimmer, as the same amount of light is spread over a larger area.\n",
    "\n",
    "### The Revolution of Lenses\n",
    "\n",
    "The introduction of lenses in the 17th century marked a crucial advancement in imaging technology. Lenses could gather more light than a pinhole while maintaining image sharpness. The behavior of lenses is described by the thin lens equation:\n",
    "\n",
    "$\\frac{1}{f} = \\frac{1}{d_o} + \\frac{1}{d_i}$\n",
    "\n",
    "where f is the focal length of the lens (a measure of how strongly it bends light). This equation helps us understand where to position our lens to get a sharp image. The magnification (M) produced by the lens is:\n",
    "\n",
    "$M = -\\frac{d_i}{d_o}$\n",
    "\n",
    "The negative sign indicates that the image is inverted.\n",
    "\n",
    "### The Chemical Era: Making Images Permanent\n",
    "\n",
    "The next major breakthrough came in the 1820s when Nicéphore Niépce created the first permanent photograph through a process called heliography. The basic chemical reaction involved silver nitrate (AgNO₃) being reduced to silver by light:\n",
    "\n",
    "$AgNO_3 + \\text{light} → Ag + NO_2 + O_2$\n",
    "\n",
    "This process took about 8 hours of exposure—imagine having to sit completely still for that long to have your portrait taken! The development of the daguerreotype in 1839 by Louis Daguerre significantly reduced exposure times to 10-20 minutes, making photography more practical.\n",
    "\n",
    "### The Electronic Age\n",
    "\n",
    "The transition to electronic imaging began with the discovery of the photoelectric effect, which Einstein explained in 1905. When light hits certain materials, it can knock electrons loose, generating an electric current. The energy of these ejected electrons (E_kinetic) is described by:\n",
    "\n",
    "$E_{kinetic} = hν - φ$\n",
    "\n",
    "where φ (phi) is called the work function—the minimum energy needed to free an electron from the material's surface. This discovery laid the foundation for all modern electronic imaging devices.\n",
    "\n",
    "The first practical electronic imaging devices were television camera tubes. These produced a photocurrent (I_photo) proportional to the incident light energy (E):\n",
    "\n",
    "$I_{photo} = ρE$\n",
    "\n",
    "where ρ (rho) is the quantum efficiency—the percentage of incoming photons that successfully generate electrons.\n",
    "\n",
    "### The Digital Revolution: CCDs and CMOS Sensors\n",
    "\n",
    "The invention of the Charge-Coupled Device (CCD) in 1969 marked the beginning of modern digital imaging. CCDs convert light into electrical charges with remarkable efficiency—up to 90% of incoming photons generate usable signals. The quality of a CCD image depends on its signal-to-noise ratio (SNR):\n",
    "\n",
    "$SNR = \\frac{N_e}{\\sqrt{N_e + n_d t + n_r^2}}$\n",
    "\n",
    "This equation includes three sources of noise:\n",
    "- N_e: the signal itself (photon shot noise)\n",
    "- n_d t: dark current (thermal noise) accumulating over time t\n",
    "- n_r: read noise from the electronics\n",
    "\n",
    "CMOS sensors, which emerged in the 1990s, use a different architecture where each pixel has its own amplifier. A key parameter is the fill factor (FF):\n",
    "\n",
    "$FF = \\frac{A_{photosensitive}}{A_{pixel}} × 100\\%$\n",
    "\n",
    "This represents what percentage of each pixel actually captures light, with the rest being taken up by electronic components.\n",
    "\n",
    "## Modern Digital Imaging\n",
    "\n",
    "The conversion of light into digital data involves several steps. First, the continuous light signal must be sampled at discrete points. The Nyquist-Shannon sampling theorem tells us how finely we need to sample to avoid losing information:\n",
    "\n",
    "$f_s > 2f_{max}$\n",
    "\n",
    "The sampling frequency (f_s) must be more than twice the highest frequency (f_max) present in the signal. In imaging terms, this means our pixels must be small enough to capture the finest details we want to resolve.\n",
    "\n",
    "The dynamic range (DR) of a digital imaging system—its ability to capture both very bright and very dark areas—is given by:\n",
    "\n",
    "$DR = 20\\log_{10}(\\frac{FWC}{n_r})$\n",
    "\n",
    "where FWC is the full well capacity (how many electrons each pixel can hold) and n_r is the read noise. The logarithmic scale (in decibels) helps us handle the wide range of values involved.\n",
    "\n",
    "Modern imaging systems convert the continuous world into discrete samples through both spatial sampling and intensity quantization. The spatial resolution is determined by:\n",
    "\n",
    "$p = \\frac{FOV}{N}$\n",
    "\n",
    "where FOV is the field of view and N is the number of pixels. Meanwhile, intensity quantization divides the continuous range of light intensities into discrete levels—typically 256 levels for 8-bit imaging, or 65,536 levels for 16-bit imaging used in scientific applications.\n",
    "\n",
    "Through this journey from the simple camera obscura to modern digital sensors, we see how our understanding of light, chemistry, and electronics has evolved, leading to increasingly sophisticated ways of capturing and representing images. Each advancement built upon previous knowledge, creating the rich field of imaging science we have today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Images: Mathematical Foundations <a id='digital'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. From Continuous to Digital: The Nature of Digital Images\n",
    "\n",
    "When we look at the world around us, we see a continuous flow of visual information. Light reflects off objects in infinite variations of intensity and color, creating what mathematicians call a continuous function. However, digital cameras and computers can only work with discrete, finite values. This fundamental transition from continuous to discrete is at the heart of digital imaging.\n",
    "\n",
    "### 1.1 The Mathematical Framework\n",
    "\n",
    "In the continuous world, an image can be represented as a function $f(x,y)$, where:\n",
    "- x and y are real numbers representing spatial coordinates ($x,y \\in \\mathbb{R}^2$)\n",
    "- $f$ maps these coordinates to intensity values\n",
    "- Both the coordinates and intensity values can take any real value within their range\n",
    "\n",
    "For example, if you were to measure the brightness of sunlight reflecting off a white wall, you could position your sensor at any point (x,y), and the intensity value could be any real number within the sensitivity range of your sensor.\n",
    "\n",
    "When we create a digital image, we must transform this continuous function into a discrete one. This transformation is represented as:\n",
    "\n",
    "$f(x,y) \\rightarrow I[m,n]$\n",
    "\n",
    "Here, the square brackets [m,n] indicate we're now working with discrete coordinates. This means:\n",
    "- m and n are integers ($[m,n] \\in \\mathbb{Z}^2$)\n",
    "- I maps to a finite set of possible intensity values\n",
    "- Both spatial coordinates and intensity values must be quantized\n",
    "\n",
    "### 1.2 The Sampling Process\n",
    "\n",
    "The transition from continuous to discrete involves two fundamental processes: spatial sampling and amplitude quantization.\n",
    "\n",
    "#### Spatial Sampling\n",
    "Spatial sampling converts continuous coordinates into discrete pixel positions:\n",
    "\n",
    "$x = m\\Delta x, y = n\\Delta y$\n",
    "\n",
    "where:\n",
    "- $\\Delta x$ and $\\Delta y$ are the sampling intervals (essentially, the pixel size)\n",
    "- m and n are integer indices\n",
    "- Each pixel represents a small region of the continuous scene\n",
    "\n",
    "To understand this, imagine placing a grid over a continuous image. Each grid cell becomes a pixel, and we need to decide how to represent the continuous information within that cell as a single value.\n",
    "\n",
    "#### Amplitude Quantization\n",
    "After sampling the spatial coordinates, we must also quantize the intensity values. This process is described by:\n",
    "\n",
    "$I[m,n] = Q\\left(\\lfloor\\frac{f(m\\Delta x, n\\Delta y)}{q}\\rfloor\\right)$\n",
    "\n",
    "where:\n",
    "- Q is the quantization operator\n",
    "- q is the quantization step size\n",
    "- $\\lfloor \\cdot \\rfloor$ represents the floor function\n",
    "- The result is a discrete intensity value\n",
    "\n",
    "For example, in 8-bit imaging, we quantize continuous intensity values into 256 discrete levels (0-255). Think of this as dividing a smooth gradient into 256 distinct steps.\n",
    "\n",
    "## 2. The Matrix Nature of Digital Images\n",
    "\n",
    "### 2.1 Understanding Image Matrices\n",
    "\n",
    "A digital image is stored and manipulated as a matrix of numbers. For a grayscale image of height M and width N, this matrix can be written as:\n",
    "\n",
    "$I = \\begin{bmatrix} \n",
    "I[0,0] & I[0,1] & \\cdots & I[0,N-1] \\\\\n",
    "I[1,0] & I[1,1] & \\cdots & I[1,N-1] \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "I[M-1,0] & I[M-1,1] & \\cdots & I[M-1,N-1]\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Each element I[m,n] represents a pixel value, where:\n",
    "- Rows (m) run from 0 to M-1\n",
    "- Columns (n) run from 0 to N-1\n",
    "- The origin [0,0] is typically at the top-left corner\n",
    "\n",
    "This matrix representation is not just a convenient way to store image data—it enables us to apply powerful mathematical operations to manipulate and analyze images.\n",
    "\n",
    "### 2.2 Understanding Pixel Values\n",
    "\n",
    "The range of possible pixel values depends on the bit depth of the image. For a bit depth of b, each pixel can take one of $2^b$ possible values:\n",
    "\n",
    "- 8-bit: $I[m,n] \\in \\{0,1,\\dots,255\\}$\n",
    "  - Common in everyday photography\n",
    "  - Sufficient for most display purposes\n",
    "  - 256 possible intensity levels\n",
    "\n",
    "- 12-bit: $I[m,n] \\in \\{0,1,\\dots,4095\\}$\n",
    "  - Often used in scientific imaging\n",
    "  - Provides finer intensity discrimination\n",
    "  - 4,096 possible intensity levels\n",
    "\n",
    "- 16-bit: $I[m,n] \\in \\{0,1,\\dots,65535\\}$\n",
    "  - Used in high-end scientific applications\n",
    "  - Captures subtle intensity variations\n",
    "  - 65,536 possible intensity levels\n",
    "\n",
    "The choice of bit depth involves a trade-off between precision and storage requirements. More bits mean better precision but larger file sizes.\n",
    "\n",
    "### 2.3 Pixel Neighborhoods and Connectivity\n",
    "\n",
    "In digital image processing, we often need to consider how pixels relate to their neighbors. There are two main types of pixel connectivity:\n",
    "\n",
    "1. 4-connectivity: A pixel at position [m,n] has four immediate neighbors:\n",
    "   - $\\{I[m±1,n], I[m,n±1]\\}$\n",
    "   - These are the pixels directly above, below, left, and right\n",
    "\n",
    "2. 8-connectivity: Includes the four diagonal neighbors as well:\n",
    "   - $\\{I[m±1,n±1], I[m±1,n], I[m,n±1]\\}$\n",
    "   - This includes all eight surrounding pixels\n",
    "\n",
    "Understanding connectivity is crucial for operations like:\n",
    "- Edge detection\n",
    "- Region growing\n",
    "- Object labeling\n",
    "- Contour tracing\n",
    "\n",
    "## 3. Color Images and Multi-Channel Representation\n",
    "\n",
    "### 3.1 The RGB Color Space\n",
    "\n",
    "While grayscale images use a single matrix, color images require three matrices—one for each color channel. This creates a 3D tensor $I \\in \\mathbb{R}^{M \\times N \\times 3}$. For each pixel location [m,n], we have:\n",
    "\n",
    "$I[m,n,c] = \\begin{cases}\n",
    "R[m,n] & \\text{if } c = 0 \\text{ (Red channel)} \\\\\n",
    "G[m,n] & \\text{if } c = 1 \\text{ (Green channel)} \\\\\n",
    "B[m,n] & \\text{if } c = 2 \\text{ (Blue channel)}\n",
    "\\end{cases}$\n",
    "\n",
    "This representation allows us to:\n",
    "1. Work with each color channel independently\n",
    "2. Combine channels in different ways\n",
    "3. Transform between different color spaces\n",
    "\n",
    "### 3.2 Understanding Color Channels\n",
    "\n",
    "Each color channel is its own matrix:\n",
    "- Red channel: $R \\in \\mathbb{R}^{M \\times N}$\n",
    "- Green channel: $G \\in \\mathbb{R}^{M \\times N}$\n",
    "- Blue channel: $B \\in \\mathbb{R}^{M \\times N}$\n",
    "\n",
    "Together, they form the complete color image: $I = \\{R,G,B\\}$\n",
    "\n",
    "### 3.3 Color as a Vector Space\n",
    "\n",
    "We can think of each pixel's color as a vector in 3D space:\n",
    "\n",
    "$\\vec{p}[m,n] = \\begin{bmatrix} R[m,n] \\\\ G[m,n] \\\\ B[m,n] \\end{bmatrix}$\n",
    "\n",
    "This vector representation helps us understand:\n",
    "- Color similarity (vector distance)\n",
    "- Color mixing (vector addition)\n",
    "- Intensity scaling (vector multiplication)\n",
    "\n",
    "For a given bit depth b:\n",
    "- Each channel ranges from 0 to $2^b-1$\n",
    "- The total number of possible colors is $(2^b)^3$\n",
    "- For 8-bit color, this means 16.7 million possible colors\n",
    "\n",
    "### 3.4 Converting to Grayscale\n",
    "\n",
    "When we need to convert a color image to grayscale, we use a weighted sum of the color channels:\n",
    "\n",
    "$I_{gray}[m,n] = 0.299R[m,n] + 0.587G[m,n] + 0.114B[m,n]$\n",
    "\n",
    "These specific weights are chosen to match human perception of brightness in different colors. The green channel has the highest weight because human vision is most sensitive to green light.\n",
    "\n",
    "## 4. Practical Considerations in Digital Imaging\n",
    "\n",
    "### 4.1 Memory Requirements\n",
    "\n",
    "Understanding memory usage is crucial for practical applications. For an M × N image with bit depth b:\n",
    "\n",
    "- Grayscale images require: $M \\times N \\times b$ bits\n",
    "- Color images require: $M \\times N \\times 3b$ bits\n",
    "\n",
    "For example, a 1000 × 1000 pixel color image at 8 bits per channel requires:\n",
    "1000 × 1000 × 3 × 8 = 24 million bits = 3 megabytes\n",
    "\n",
    "### 4.2 Data Types and Precision\n",
    "\n",
    "Different numerical representations offer different trade-offs:\n",
    "\n",
    "1. Integer Types:\n",
    "   - uint8: 0 to 255\n",
    "     - Compact storage\n",
    "     - Limited range\n",
    "   - uint16: 0 to 65535\n",
    "     - Better precision\n",
    "     - Larger file size\n",
    "\n",
    "2. Floating Point:\n",
    "   - float32: ~7 decimal digits precision\n",
    "     - Good for intermediate calculations\n",
    "     - Allows values outside [0,1] range\n",
    "   - float64: ~15 decimal digits precision\n",
    "     - Highest precision\n",
    "     - Largest memory requirement\n",
    "\n",
    "The choice of data type affects:\n",
    "- Precision of calculations\n",
    "- Memory usage\n",
    "- Processing speed\n",
    "- Compatibility with different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Image Operations and Transformations <a id='transformations'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Point Operations: Manipulating Individual Pixels\n",
    "\n",
    "### 1.1 Understanding Point Operations\n",
    "\n",
    "Point operations are the simplest form of image manipulation, where we modify each pixel's value independently of its neighbors. These operations follow a fundamental principle: the value of each output pixel depends only on the value of the corresponding input pixel at the same location. Mathematically, we express this as:\n",
    "\n",
    "$g[m,n] = T(f[m,n])$\n",
    "\n",
    "where:\n",
    "- $f[m,n]$ is the input image value at position [m,n]\n",
    "- $g[m,n]$ is the output image value at the same position\n",
    "- $T(\\cdot)$ is called the transfer function or point transformation function\n",
    "\n",
    "This seemingly simple framework allows us to perform a wide variety of useful image adjustments. Think of T as a recipe that tells us how to cook each pixel - and every pixel follows the same recipe independently.\n",
    "\n",
    "### 1.2 Linear Intensity Transformations\n",
    "\n",
    "The most basic point operation is the linear transformation, described by:\n",
    "\n",
    "$g[m,n] = \\alpha f[m,n] + \\beta$\n",
    "\n",
    "This equation has two parameters that control different aspects of the image:\n",
    "- α (alpha) controls contrast\n",
    "  - α > 1 increases contrast\n",
    "  - 0 < α < 1 reduces contrast\n",
    "  - α < 0 inverts and scales the image\n",
    "- β (beta) controls brightness\n",
    "  - β > 0 brightens the image\n",
    "  - β < 0 darkens the image\n",
    "\n",
    "For example, if we have a pixel with value 100, and we set α = 1.5 and β = 20, the new pixel value would be:\n",
    "1.5 × 100 + 20 = 170\n",
    "\n",
    "This transformation has several important properties:\n",
    "1. It preserves relative differences between pixels (proportionally)\n",
    "2. It can be inverted (reversed) when α ≠ 0 using:\n",
    "   $f[m,n] = \\frac{g[m,n] - \\beta}{\\alpha}$\n",
    "3. It can cause \"clipping\" if values exceed the valid range (e.g., 0-255 for 8-bit images)\n",
    "\n",
    "### 1.3 Gamma Correction\n",
    "\n",
    "Sometimes a linear transformation isn't enough, particularly when we need to adjust the brightness of mid-tones without affecting the extremes as much. This is where gamma correction comes in:\n",
    "\n",
    "$g[m,n] = c(f[m,n])^\\gamma$\n",
    "\n",
    "where:\n",
    "- c is a scaling constant (often 1)\n",
    "- γ (gamma) controls the shape of the transformation\n",
    "- For proper calculation, pixel values must be normalized to the range [0,1]\n",
    "\n",
    "The gamma parameter has different effects:\n",
    "- γ < 1: Enhances detail in dark regions\n",
    "  - Example: γ = 0.5 brings out shadow detail\n",
    "- γ > 1: Enhances detail in bright regions\n",
    "  - Example: γ = 2.2 is often used to compensate for display characteristics\n",
    "- γ = 1: No change (linear response)\n",
    "\n",
    "To understand why this works, consider how the function behaves:\n",
    "- For small input values (dark pixels), when γ < 1, the output increases more rapidly than the input\n",
    "- For large input values (bright pixels), the output increases more slowly\n",
    "- This non-linear behavior helps match human perception of brightness\n",
    "\n",
    "### 1.4 Intensity Window/Level Adjustment\n",
    "\n",
    "In scientific imaging, we often need to focus on a specific range of intensity values. Window/level adjustment (also called contrast stretching) allows us to do this:\n",
    "\n",
    "$g[m,n] = \\begin{cases} \n",
    "0 & f[m,n] \\leq l - \\frac{w}{2} \\\\\n",
    "\\frac{f[m,n] - (l - \\frac{w}{2})}{w} & l - \\frac{w}{2} < f[m,n] < l + \\frac{w}{2} \\\\\n",
    "1 & f[m,n] \\geq l + \\frac{w}{2}\n",
    "\\end{cases}$\n",
    "\n",
    "where:\n",
    "- w is the window width (range of intensities to display)\n",
    "- l is the center level (center of the range of interest)\n",
    "\n",
    "This operation:\n",
    "1. Sets all values below (l - w/2) to black (0)\n",
    "2. Sets all values above (l + w/2) to white (1)\n",
    "3. Linearly scales values in between\n",
    "\n",
    "This is particularly useful in medical and scientific imaging where we might want to focus on specific intensity ranges, like different tissue types in medical images.\n",
    "\n",
    "## 2. Understanding Image Histograms\n",
    "\n",
    "### 2.1 What is a Histogram?\n",
    "\n",
    "A histogram is a graphical representation of the distribution of pixel intensities in an image. For an image with L possible intensity levels, the histogram h(k) counts how many pixels have each intensity value k:\n",
    "\n",
    "$h(k) = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} \\delta(f[m,n] - k)$\n",
    "\n",
    "where:\n",
    "- k ranges from 0 to L-1 (e.g., 0-255 for 8-bit images)\n",
    "- δ(x) is the Kronecker delta function:\n",
    "  $\\delta(x) = \\begin{cases} 1 & \\text{if } x = 0 \\\\ 0 & \\text{otherwise} \\end{cases}$\n",
    "\n",
    "To make histograms comparable between images of different sizes, we often normalize them to get the probability distribution:\n",
    "\n",
    "$p(k) = \\frac{h(k)}{MN}$\n",
    "\n",
    "where MN is the total number of pixels in the image.\n",
    "\n",
    "### 2.2 Histogram Equalization\n",
    "\n",
    "Histogram equalization is a powerful technique that automatically enhances image contrast by spreading out the most frequent intensity values. The transformation function is:\n",
    "\n",
    "$T(k) = (L-1)\\sum_{i=0}^k p(i) = (L-1)CDF(k)$\n",
    "\n",
    "where:\n",
    "- L is the number of possible intensity levels\n",
    "- CDF(k) is the cumulative distribution function\n",
    "\n",
    "For continuous values, this becomes:\n",
    "$s = T(r) = (L-1)\\int_0^r p_r(w)dw$\n",
    "\n",
    "The process works by:\n",
    "1. Computing the histogram\n",
    "2. Creating a cumulative sum\n",
    "3. Scaling the result to the desired range\n",
    "4. Mapping each input intensity to its new value\n",
    "\n",
    "Properties of histogram equalization:\n",
    "- Creates an approximately uniform distribution of intensities\n",
    "- Maximizes the entropy of the image\n",
    "- Enhances global contrast\n",
    "- Works best on images with poor contrast distribution\n",
    "\n",
    "## 3. Geometric Transformations\n",
    "\n",
    "### 3.1 Understanding Affine Transformations\n",
    "\n",
    "Affine transformations are operations that preserve lines and parallelism. In homogeneous coordinates, they can be represented as a matrix multiplication:\n",
    "\n",
    "$\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "a_{11} & a_{12} & t_x \\\\\n",
    "a_{21} & a_{22} & t_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "This compact notation encompasses several important transformations:\n",
    "\n",
    "1. Translation (moving the image):\n",
    "$\\begin{bmatrix} \n",
    "1 & 0 & t_x \\\\\n",
    "0 & 1 & t_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- tx moves the image horizontally\n",
    "- ty moves it vertically\n",
    "- No change in shape or size\n",
    "\n",
    "2. Rotation (by angle θ):\n",
    "$\\begin{bmatrix} \n",
    "\\cos\\theta & -\\sin\\theta & 0 \\\\\n",
    "\\sin\\theta & \\cos\\theta & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- Rotates around the origin\n",
    "- Preserves distances and angles\n",
    "- Positive θ is counterclockwise\n",
    "\n",
    "3. Scaling:\n",
    "$\\begin{bmatrix} \n",
    "s_x & 0 & 0 \\\\\n",
    "0 & s_y & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- sx scales horizontally\n",
    "- sy scales vertically\n",
    "- Uniform scaling when sx = sy\n",
    "\n",
    "### 3.2 Interpolation Methods\n",
    "\n",
    "When we perform geometric transformations, we often need to compute pixel values at non-integer coordinates. This requires interpolation:\n",
    "\n",
    "1. Nearest Neighbor:\n",
    "   $g[m,n] = f[\\lfloor x \\rceil, \\lfloor y \\rceil]$\n",
    "   \n",
    "   - Simplest method\n",
    "   - Creates blocky results\n",
    "   - No new intensity values\n",
    "   - Fast but low quality\n",
    "\n",
    "2. Bilinear Interpolation:\n",
    "   Let α = x - ⌊x⌋, β = y - ⌊y⌋\n",
    "   \n",
    "   $g[m,n] = (1-\\alpha)(1-\\beta)f[\\lfloor x \\rfloor, \\lfloor y \\rfloor] + \\\\ \n",
    "   \\alpha(1-\\beta)f[\\lceil x \\rceil, \\lfloor y \\rfloor] + \\\\\n",
    "   (1-\\alpha)\\beta f[\\lfloor x \\rfloor, \\lceil y \\rceil] + \\\\\n",
    "   \\alpha\\beta f[\\lceil x \\rceil, \\lceil y \\rceil]$\n",
    "\n",
    "   - Weighted average of 4 nearest pixels\n",
    "   - Smoother results than nearest neighbor\n",
    "   - Can create new intensity values\n",
    "   - Good balance of quality and speed\n",
    "\n",
    "3. Bicubic Interpolation:\n",
    "   Uses cubic convolution kernel:\n",
    "   $h(x) = \\begin{cases}\n",
    "   1 - 2|x|^2 + |x|^3 & 0 \\leq |x| < 1 \\\\\n",
    "   4 - 8|x| + 5|x|^2 - |x|^3 & 1 \\leq |x| < 2 \\\\\n",
    "   0 & 2 \\leq |x|\n",
    "   \\end{cases}$\n",
    "\n",
    "   - Uses 16 surrounding pixels\n",
    "   - Highest quality results\n",
    "   - Computationally intensive\n",
    "   - Can create overshoots (ringing)\n",
    "\n",
    "## 4. Frequency Domain Analysis\n",
    "\n",
    "### 4.1 The Discrete Fourier Transform\n",
    "\n",
    "The Fourier transform allows us to analyze images in terms of their frequency components. For a digital image, we use the Discrete Fourier Transform (DFT):\n",
    "\n",
    "Forward transform:\n",
    "$F[u,v] = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} f[m,n]e^{-j2\\pi(\\frac{um}{M} + \\frac{vn}{N})}$\n",
    "\n",
    "Inverse transform:\n",
    "$f[m,n] = \\frac{1}{MN}\\sum_{u=0}^{M-1} \\sum_{v=0}^{N-1} F[u,v]e^{j2\\pi(\\frac{um}{M} + \\frac{vn}{N})}$\n",
    "\n",
    "Key properties:\n",
    "1. Linearity: \n",
    "   $\\mathcal{F}\\{af_1 + bf_2\\} = a\\mathcal{F}\\{f_1\\} + b\\mathcal{F}\\{f_2\\}$\n",
    "   - Allows analysis of complex images as sum of simpler components\n",
    "\n",
    "2. Translation:\n",
    "   $f[m-m_0,n-n_0] \\leftrightarrow F[u,v]e^{-j2\\pi(\\frac{um_0}{M} + \\frac{vn_0}{N})}$\n",
    "   - Spatial shifts become phase changes\n",
    "\n",
    "3. Rotation:\n",
    "   - Rotating the image rotates its frequency spectrum\n",
    "   - Angle of rotation is preserved\n",
    "\n",
    "4. Scaling:\n",
    "   - Inverse relationship between spatial and frequency scaling\n",
    "   - Stretching in space compresses in frequency\n",
    "\n",
    "### 4.2 Understanding the Frequency Domain\n",
    "\n",
    "The Fourier transform gives us three important representations:\n",
    "\n",
    "1. Power Spectrum:\n",
    "   $P[u,v] = |F[u,v]|^2$\n",
    "   - Shows energy distribution across frequencies\n",
    "   - Independent of phase\n",
    "   - Often displayed logarithmically\n",
    "\n",
    "2. Phase Spectrum:\n",
    "   $\\phi[u,v] = \\tan^{-1}\\left(\\frac{\\Im\\{F[u,v]\\}}{\\Re\\{F[u,v]\\}}\\right)$\n",
    "   - Contains structural information\n",
    "   - Critical for image reconstruction\n",
    "   - Often more important than magnitude\n",
    "\n",
    "3. Magnitude Spectrum (visualized):\n",
    "   $D[u,v] = \\log(1 + |F[u,v]|)$\n",
    "   - Logarithmic scaling helps visualization\n",
    "   - Shows frequency content\n",
    "   - Center represents DC (average) component\n",
    "\n",
    "Understanding these representations helps us:\n",
    "- Analyze image content\n",
    "- Design filters\n",
    "- Identify periodic patterns\n",
    "- Remove noise\n",
    "- Compress images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Enhancement and Restoration <a id='restoration'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Noise in Digital Images\n",
    "\n",
    "When we capture a digital image, the result is never a perfect representation of the scene. Instead, what we get is the true image contaminated by various forms of noise. To understand this mathematically, we model a noisy image using the equation:\n",
    "\n",
    "$g[m,n] = f[m,n] + \\eta[m,n]$\n",
    "\n",
    "Here, $f[m,n]$ represents the \"true\" image we wanted to capture, $\\eta[m,n]$ is the noise that contaminated our image during acquisition, and $g[m,n]$ is the actual noisy image we obtained. The square brackets [m,n] indicate that we're working with discrete pixel coordinates, where m represents the row and n represents the column in our image matrix.\n",
    "\n",
    "### 1.1 Common Types of Noise and Their Mathematical Models\n",
    "\n",
    "Different imaging conditions and hardware characteristics lead to different types of noise. Understanding these noise models is crucial for choosing the right restoration technique.\n",
    "\n",
    "#### Gaussian (Normal) Noise\n",
    "The most commonly encountered form of noise follows a Gaussian distribution, described by the probability density function:\n",
    "\n",
    "$p(z) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(z-\\mu)^2}{2\\sigma^2}}$\n",
    "\n",
    "This equation might look intimidating, but it's describing something quite straightforward: the probability ($p(z)$) of a pixel being corrupted by a noise value $z$. The parameter μ represents the average noise value (often zero), and σ determines how \"spread out\" the noise is. Gaussian noise typically arises from electronic noise in the sensor or during transmission.\n",
    "\n",
    "#### Poisson (Shot) Noise\n",
    "In low-light imaging or when working with fluorescent samples, we often encounter Poisson noise. Unlike Gaussian noise, Poisson noise is signal-dependent, meaning brighter parts of the image will have more noise. It follows the probability distribution:\n",
    "\n",
    "$p(k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$\n",
    "\n",
    "where λ is both the mean and variance of the distribution. This relationship between mean and variance ($\\sigma^2 = \\lambda$) is a key characteristic that helps us identify and handle Poisson noise.\n",
    "\n",
    "#### Salt-and-Pepper Noise\n",
    "This type of noise manifests as random white and black pixels in the image. It's mathematically described as:\n",
    "\n",
    "$p(z) = \\begin{cases}\n",
    "P_a & \\text{for } z = a \\text{ (pepper, typically 0)} \\\\\n",
    "P_b & \\text{for } z = b \\text{ (salt, typically 255)} \\\\\n",
    "1-P_a-P_b & \\text{for original pixel value}\n",
    "\\end{cases}$\n",
    "\n",
    "This noise often results from transmission errors or faulty pixels in the sensor.\n",
    "\n",
    "### 1.2 Measuring Image Quality: Signal-to-Noise Ratio\n",
    "\n",
    "To quantify how much noise is present in an image, we use the Signal-to-Noise Ratio (SNR):\n",
    "\n",
    "$SNR = 10\\log_{10}\\left(\\frac{\\sigma_f^2}{\\sigma_\\eta^2}\\right)$\n",
    "\n",
    "where $\\sigma_f^2$ is the variance of the true image signal and $\\sigma_\\eta^2$ is the variance of the noise. The logarithmic scale (in decibels) helps us handle the wide range of values we encounter in practice. A higher SNR indicates a cleaner image.\n",
    "\n",
    "## 2. Linear Filtering: The Foundation of Image Enhancement\n",
    "\n",
    "Linear filtering is one of the most fundamental approaches to image enhancement. It works by replacing each pixel with a weighted sum of its neighbors.\n",
    "\n",
    "### 2.1 Understanding Convolution\n",
    "\n",
    "The mathematical operation underlying linear filtering is convolution. For digital images, we use the discrete convolution formula:\n",
    "\n",
    "$g[m,n] = (f * h)[m,n] = \\sum_{k=-a}^a \\sum_{l=-b}^b h[k,l]f[m-k,n-l]$\n",
    "\n",
    "Let's break this down:\n",
    "- $h[k,l]$ is our filter kernel (also called mask or window)\n",
    "- The sums run from -a to a and -b to b, defining the size of our kernel\n",
    "- For each output pixel g[m,n], we:\n",
    "  1. Center the kernel at position [m,n]\n",
    "  2. Multiply each kernel value by the corresponding image pixel\n",
    "  3. Sum all these products to get the filtered pixel value\n",
    "\n",
    "### 2.2 Gaussian Filtering: Smoothing with a Purpose\n",
    "\n",
    "Gaussian filtering is particularly effective for reducing random noise while preserving image structure. The kernel is based on the 2D Gaussian function:\n",
    "\n",
    "$h[k,l] = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{k^2+l^2}{2\\sigma^2}}$\n",
    "\n",
    "The parameter σ controls the amount of smoothing:\n",
    "- Larger σ values create more blurring\n",
    "- Smaller σ values preserve more detail\n",
    "\n",
    "In practice, we use a discrete approximation:\n",
    "$h[k,l] = Ce^{-\\frac{k^2+l^2}{2\\sigma^2}}$, for $k,l \\in [-a,a]$\n",
    "\n",
    "where C is a normalization constant ensuring the kernel sums to 1:\n",
    "$C = \\frac{1}{\\sum_{k=-a}^a \\sum_{l=-a}^a e^{-\\frac{k^2+l^2}{2\\sigma^2}}}$\n",
    "\n",
    "### 2.3 Mean Filtering: Simple but Effective\n",
    "\n",
    "The mean filter is the simplest form of linear filtering, where all kernel values are equal:\n",
    "\n",
    "$h[k,l] = \\frac{1}{(2a+1)(2b+1)}$\n",
    "\n",
    "This uniform weighting means each pixel is replaced by the average of its neighborhood. While simple, this approach has some important properties:\n",
    "- It preserves the average intensity (DC component) of the image\n",
    "- It reduces random noise effectively\n",
    "- However, it tends to blur edges significantly\n",
    "\n",
    "## 3. Nonlinear Filtering: Beyond Simple Averaging\n",
    "\n",
    "Sometimes linear filtering isn't enough, particularly when we need to preserve edges or handle certain types of noise. This is where nonlinear filters come in.\n",
    "\n",
    "### 3.1 Median Filtering: Robust Noise Removal\n",
    "\n",
    "The median filter works by replacing each pixel with the median value in its neighborhood:\n",
    "\n",
    "$g[m,n] = \\text{median}\\{f[i,j] : [i,j] \\in \\mathcal{N}_{m,n}\\}$\n",
    "\n",
    "Here, $\\mathcal{N}_{m,n}$ represents the neighborhood around pixel [m,n]. The median filter is particularly effective because:\n",
    "- It completely removes salt-and-pepper noise\n",
    "- It preserves edges better than linear filters\n",
    "- It doesn't introduce new pixel values\n",
    "\n",
    "### 3.2 Bilateral Filtering: Edge-Preserving Smoothing\n",
    "\n",
    "The bilateral filter is a sophisticated approach that combines spatial and intensity information:\n",
    "\n",
    "$g[m,n] = \\frac{\\sum_{k,l} f[k,l]w[k,l,m,n]}{\\sum_{k,l} w[k,l,m,n]}$\n",
    "\n",
    "The weight function $w[k,l,m,n]$ has two components:\n",
    "\n",
    "$w[k,l,m,n] = e^{-\\frac{(k-m)^2+(l-n)^2}{2\\sigma_d^2}}e^{-\\frac{(f[k,l]-f[m,n])^2}{2\\sigma_r^2}}$\n",
    "\n",
    "This complex-looking equation actually does something quite intuitive:\n",
    "- The first exponential term reduces the influence of distant pixels\n",
    "- The second exponential term reduces the influence of pixels with very different intensities\n",
    "- $\\sigma_d$ controls the spatial extent of the filter\n",
    "- $\\sigma_r$ controls how much intensity difference is allowed\n",
    "\n",
    "## 4. Image Deconvolution: Recovering the True Image\n",
    "\n",
    "Image deconvolution attempts to reverse the blurring that occurs during image capture. The process starts with the blur model:\n",
    "\n",
    "$g[m,n] = (h * f)[m,n] + \\eta[m,n]$\n",
    "\n",
    "In the frequency domain, this becomes:\n",
    "$G[u,v] = H[u,v]F[u,v] + N[u,v]$\n",
    "\n",
    "where:\n",
    "- $h[m,n]$ is the Point Spread Function (PSF) describing how the imaging system blurs points\n",
    "- $H[u,v]$ is the Optical Transfer Function (the Fourier transform of the PSF)\n",
    "- Capital letters represent Fourier transforms of their lowercase counterparts\n",
    "\n",
    "### 4.1 Wiener Deconvolution\n",
    "\n",
    "Wiener deconvolution provides an optimal solution in the presence of noise:\n",
    "\n",
    "$\\hat{F}[u,v] = \\frac{H^*[u,v]}{|H[u,v]|^2 + K}G[u,v]$\n",
    "\n",
    "This formula:\n",
    "- Uses the complex conjugate $H^*[u,v]$ to reverse the blur\n",
    "- Includes the term K (noise-to-signal power ratio) to prevent noise amplification\n",
    "- Provides the estimated true image spectrum $\\hat{F}[u,v]$\n",
    "\n",
    "### 4.2 Richardson-Lucy Deconvolution\n",
    "\n",
    "For images with Poisson noise (common in microscopy), the Richardson-Lucy algorithm often works better:\n",
    "\n",
    "$f^{(t+1)}[m,n] = f^{(t)}[m,n]\\left(h[-m,-n] * \\frac{g[m,n]}{(h * f^{(t)})[m,n]}\\right)$\n",
    "\n",
    "This iterative approach:\n",
    "- Ensures all pixel values remain positive\n",
    "- Converges to the maximum likelihood solution\n",
    "- Often provides better results than Wiener filtering for low-light images\n",
    "\n",
    "## 5. Frequency Domain Enhancement\n",
    "\n",
    "Working in the frequency domain often provides more control over enhancement operations.\n",
    "\n",
    "### 5.1 Basic Frequency Domain Filters\n",
    "\n",
    "The ideal low-pass filter:\n",
    "$H[u,v] = \\begin{cases}\n",
    "1 & \\text{if } \\sqrt{u^2 + v^2} \\leq D_0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "And its high-pass counterpart:\n",
    "$H[u,v] = \\begin{cases}\n",
    "0 & \\text{if } \\sqrt{u^2 + v^2} \\leq D_0 \\\\\n",
    "1 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "Here, $D_0$ is the cutoff frequency, determining which frequencies pass through the filter.\n",
    "\n",
    "### 5.2 Butterworth Filters: Smooth Frequency Response\n",
    "\n",
    "The Butterworth filter provides a smoother transition in frequency response:\n",
    "\n",
    "Low-pass:\n",
    "$H[u,v] = \\frac{1}{1 + [\\sqrt{u^2 + v^2}/D_0]^{2n}}$\n",
    "\n",
    "High-pass:\n",
    "$H[u,v] = \\frac{1}{1 + [D_0/\\sqrt{u^2 + v^2}]^{2n}}$\n",
    "\n",
    "The order n controls how sharp the frequency cutoff is:\n",
    "- Higher n values create sharper transitions\n",
    "- Lower n values provide smoother transitions\n",
    "- This flexibility often makes Butterworth filters more practical than ideal filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edge Detection and Feature Extraction: Mathematical Foundations <a id='edges'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Edges in Digital Images\n",
    "\n",
    "### 1.1 What is an Edge?\n",
    "\n",
    "At its most basic level, an edge is a place in an image where the brightness changes significantly. Think of a black circle on a white background - the boundary between black and white is an edge. To understand this mathematically, we need to quantify how quickly the brightness changes from one pixel to the next.\n",
    "\n",
    "#### Mathematical Notation Primer:\n",
    "- $f[m,n]$ represents a pixel value at position [m,n] in our image\n",
    "  - m is the row number (counting from top to bottom)\n",
    "  - n is the column number (counting from left to right)\n",
    "  - The square brackets [m,n] tell us we're working with discrete positions (whole numbers only)\n",
    "- The symbol Δ (delta) means \"change in\"\n",
    "- The symbol ∂ (partial derivative) represents rate of change with respect to one variable\n",
    "- A function f(x) shows how y changes with x. In images, this becomes f[m,n] showing how brightness changes with position\n",
    "\n",
    "### 1.2 Measuring Change: Derivatives in Images\n",
    "\n",
    "In calculus, we use derivatives to measure how quickly something changes. However, since digital images are made up of discrete pixels (we can't have pixel position 1.5), we need to approximate these derivatives using differences between neighboring pixels.\n",
    "\n",
    "#### One-Dimensional Case:\n",
    "Let's start simple. If we're moving along a single row of pixels, the simplest approximation of the derivative is:\n",
    "\n",
    "$\\frac{df}{dx} \\approx \\frac{f(x+1) - f(x)}{1} = f(x+1) - f(x)$\n",
    "\n",
    "What this means in plain English:\n",
    "1. Take a pixel value\n",
    "2. Subtract it from the next pixel's value\n",
    "3. This gives us how much the brightness changed between these pixels\n",
    "\n",
    "#### Two-Dimensional Case:\n",
    "In an image, we need to consider changes in both horizontal (x) and vertical (y) directions:\n",
    "\n",
    "Horizontal derivative (how much brightness changes left-to-right):\n",
    "$g_x[m,n] = f[m,n+1] - f[m,n]$\n",
    "\n",
    "Vertical derivative (how much brightness changes top-to-bottom):\n",
    "$g_y[m,n] = f[m+1,n] - f[m,n]$\n",
    "\n",
    "Here:\n",
    "- $g_x$ represents change in x-direction\n",
    "- $g_y$ represents change in y-direction\n",
    "- The subscripts x and y tell us which direction we're measuring\n",
    "\n",
    "### 1.3 Edge Strength and Direction\n",
    "\n",
    "Once we know how much the image changes in both directions, we can combine this information to understand edges better.\n",
    "\n",
    "#### Edge Strength (Magnitude):\n",
    "The strength of an edge is given by:\n",
    "\n",
    "$|\\nabla f[m,n]| = \\sqrt{g_x[m,n]^2 + g_y[m,n]^2}$\n",
    "\n",
    "Let's break this down:\n",
    "- The symbol ∇ (nabla) represents the gradient (combined horizontal and vertical changes)\n",
    "- The vertical bars |...| mean we're calculating magnitude (strength)\n",
    "- The square root and squares ($\\sqrt{}$ and $^2$) come from the Pythagorean theorem\n",
    "- This formula combines horizontal and vertical changes into a single measure of total change\n",
    "\n",
    "For example:\n",
    "If $g_x = 3$ (horizontal change) and $g_y = 4$ (vertical change):\n",
    "$|\\nabla f| = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5$\n",
    "\n",
    "#### Edge Direction:\n",
    "The direction of the edge is given by:\n",
    "\n",
    "$\\theta[m,n] = \\tan^{-1}\\left(\\frac{g_y[m,n]}{g_x[m,n]}\\right)$\n",
    "\n",
    "Let's understand this:\n",
    "- $\\tan^{-1}$ (inverse tangent or arctangent) converts a ratio into an angle\n",
    "- $\\frac{g_y}{g_x}$ is the ratio of vertical to horizontal change\n",
    "- θ (theta) represents the angle in radians\n",
    "- The angle tells us which way the edge is pointing\n",
    "\n",
    "For example:\n",
    "If $g_x = g_y$ (equal change in both directions):\n",
    "$\\theta = \\tan^{-1}(1) = 45°$\n",
    "This means the edge runs diagonally.\n",
    "\n",
    "### 1.4 Understanding the Numbers\n",
    "\n",
    "When we calculate these values:\n",
    "- Edge strength (|\\nabla f|) is always positive\n",
    "  - Larger numbers mean stronger edges\n",
    "  - Zero means no edge (no change in brightness)\n",
    "- Edge direction (θ) ranges from -180° to 180°\n",
    "  - 0° means horizontal edge\n",
    "  - 90° means vertical edge\n",
    "  - Other angles mean diagonal edges\n",
    "\n",
    "#### Practical Example:\n",
    "Consider a simple case where:\n",
    "- Current pixel brightness is 50\n",
    "- Right neighbor brightness is 150\n",
    "- Bottom neighbor brightness is 70\n",
    "\n",
    "Then:\n",
    "1. $g_x = 150 - 50 = 100$ (strong horizontal change)\n",
    "2. $g_y = 70 - 50 = 20$ (weak vertical change)\n",
    "3. Strength = $\\sqrt{100^2 + 20^2} = \\sqrt{10400} \\approx 102$\n",
    "4. Direction = $\\tan^{-1}(\\frac{20}{100}) \\approx 11.3°$\n",
    "\n",
    "This tells us we have:\n",
    "- A strong edge (magnitude 102)\n",
    "- Nearly horizontal (11.3° from horizontal)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
