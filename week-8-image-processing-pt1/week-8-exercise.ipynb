{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Signals Analysis - Week 8 - Image Processing Part 1\n",
    "### Table of Contents:\n",
    "- [The Evolution of Imaging: From Light to Digital Representation](#history)\n",
    "- [Digital Images: Mathematical Foundations](#digital)\n",
    "- [Basic Image Operations and Transformations](#transformations)\n",
    "- [Image Enhancement and Restoration](#restoration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Evolution of Imaging: From Light to Digital Representation <a id='history'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Physics of Light and Image Formation\n",
    "\n",
    "To understand imaging, we must first understand light itself. Light is a form of electromagnetic radiation that exhibits a fascinating dual nature: it behaves as both a wave and a particle. This duality, a cornerstone of modern physics, helps us understand how images form and how we can capture them.\n",
    "\n",
    "As a wave, light is characterized by its wavelength (λ) and frequency (ν), which are related through the speed of light (c) by the equation:\n",
    "\n",
    "$c = λν$\n",
    "\n",
    "Let's break this down: if you think of light as a wave on water, the wavelength is the distance between two consecutive wave peaks, while the frequency is how many waves pass a fixed point each second. The speed of light (approximately 3 × 10⁸ meters per second) remains constant in a vacuum, so as wavelength increases, frequency must decrease, and vice versa.\n",
    "\n",
    "The visible spectrum—the light our eyes can see—occupies only a small portion of all possible wavelengths, ranging from about 380 nanometers (violet) to 700 nanometers (red). A nanometer is incredibly small: one billionth of a meter. When light acts as particles, we call these particles photons. Each photon carries a specific amount of energy (E), determined by its wavelength:\n",
    "\n",
    "$E = \\frac{hc}{λ}$\n",
    "\n",
    "Here, h is Planck's constant (approximately 6.626 × 10⁻³⁴ joule-seconds), a fundamental constant of nature. This equation tells us something important: shorter wavelengths (like blue light) carry more energy than longer wavelengths (like red light).\n",
    "\n",
    "When light interacts with matter, several key phenomena occur, each governed by specific physical laws:\n",
    "\n",
    "1. Reflection is perhaps the most familiar—when light bounces off a surface. The law of reflection states that the angle at which light hits a surface (angle of incidence, θᵢ) equals the angle at which it bounces off (angle of reflection, θᵣ):\n",
    "\n",
    "$θ_i = θ_r$\n",
    "\n",
    "2. Refraction occurs when light passes from one medium to another, causing it to bend. This bending is described by Snell's law:\n",
    "\n",
    "$\\frac{n_1}{n_2} = \\frac{\\sin(θ_2)}{\\sin(θ_1)}$\n",
    "\n",
    "where n₁ and n₂ are the refractive indices of the two materials (essentially a measure of how much they slow down light), and θ₁ and θ₂ are the angles before and after the light bends. This is why a straw in a glass of water appears to be bent—light bends as it moves between air and water.\n",
    "\n",
    "3. Absorption occurs when materials take in light energy. The Beer-Lambert law describes how light intensity decreases as it passes through a material:\n",
    "\n",
    "$I(x) = I_0e^{-αx}$\n",
    "\n",
    "Here, I₀ is the initial light intensity, x is the distance traveled through the material, and α is the absorption coefficient (how strongly the material absorbs light). The exponential nature of this equation means that absorption happens most rapidly at first and then slows down.\n",
    "\n",
    "4. Diffraction occurs when light waves bend around obstacles or through openings. For a diffraction grating (a surface with many parallel slits), the relationship is:\n",
    "\n",
    "$\\sin(θ) = \\frac{mλ}{d}$\n",
    "\n",
    "where θ is the angle of diffraction, m is an integer representing the order of diffraction, and d is the distance between slits. Diffraction explains why we can see interference patterns and why there are fundamental limits to image resolution.\n",
    "\n",
    "## The Historical Journey of Imaging\n",
    "\n",
    "### Early Beginnings: The Camera Obscura\n",
    "\n",
    "The story of imaging begins with a simple observation: light passing through a small hole into a dark room creates an inverted image of the outside world. This phenomenon, known as the camera obscura (Latin for \"dark chamber\"), was first described in ancient China by the philosopher Mo Di around 400 BCE. However, it wasn't until the 11th century that Ibn al-Haytham provided a comprehensive mathematical treatment of this effect.\n",
    "\n",
    "The mathematics of the camera obscura is elegantly simple. If we have an object of height h₀ at distance d₀ from the pinhole, it creates an image of height hᵢ at distance dᵢ behind the pinhole:\n",
    "\n",
    "$h_i = \\frac{h_o}{d_o}d_i$\n",
    "\n",
    "This equation reveals the inherent trade-off in pinhole imaging: moving the screen farther back (increasing dᵢ) makes the image larger but dimmer, as the same amount of light is spread over a larger area.\n",
    "\n",
    "### The Revolution of Lenses\n",
    "\n",
    "The introduction of lenses in the 17th century marked a crucial advancement in imaging technology. Lenses could gather more light than a pinhole while maintaining image sharpness. The behavior of lenses is described by the thin lens equation:\n",
    "\n",
    "$\\frac{1}{f} = \\frac{1}{d_o} + \\frac{1}{d_i}$\n",
    "\n",
    "where f is the focal length of the lens (a measure of how strongly it bends light). This equation helps us understand where to position our lens to get a sharp image. The magnification (M) produced by the lens is:\n",
    "\n",
    "$M = -\\frac{d_i}{d_o}$\n",
    "\n",
    "The negative sign indicates that the image is inverted.\n",
    "\n",
    "### The Chemical Era: Making Images Permanent\n",
    "\n",
    "The next major breakthrough came in the 1820s when Nicéphore Niépce created the first permanent photograph through a process called heliography. The basic chemical reaction involved silver nitrate (AgNO₃) being reduced to silver by light:\n",
    "\n",
    "$AgNO_3 + \\text{light} → Ag + NO_2 + O_2$\n",
    "\n",
    "This process took about 8 hours of exposure—imagine having to sit completely still for that long to have your portrait taken! The development of the daguerreotype in 1839 by Louis Daguerre significantly reduced exposure times to 10-20 minutes, making photography more practical.\n",
    "\n",
    "### The Electronic Age\n",
    "\n",
    "The transition to electronic imaging began with the discovery of the photoelectric effect, which Einstein explained in 1905. When light hits certain materials, it can knock electrons loose, generating an electric current. The energy of these ejected electrons (E_kinetic) is described by:\n",
    "\n",
    "$E_{kinetic} = hν - φ$\n",
    "\n",
    "where φ (phi) is called the work function—the minimum energy needed to free an electron from the material's surface. This discovery laid the foundation for all modern electronic imaging devices.\n",
    "\n",
    "The first practical electronic imaging devices were television camera tubes. These produced a photocurrent (I_photo) proportional to the incident light energy (E):\n",
    "\n",
    "$I_{photo} = ρE$\n",
    "\n",
    "where ρ (rho) is the quantum efficiency—the percentage of incoming photons that successfully generate electrons.\n",
    "\n",
    "### The Digital Revolution: CCDs and CMOS Sensors\n",
    "\n",
    "The invention of the Charge-Coupled Device (CCD) in 1969 marked the beginning of modern digital imaging. CCDs convert light into electrical charges with remarkable efficiency—up to 90% of incoming photons generate usable signals. The quality of a CCD image depends on its signal-to-noise ratio (SNR):\n",
    "\n",
    "$SNR = \\frac{N_e}{\\sqrt{N_e + n_d t + n_r^2}}$\n",
    "\n",
    "This equation includes three sources of noise:\n",
    "- N_e: the signal itself (photon shot noise)\n",
    "- n_d t: dark current (thermal noise) accumulating over time t\n",
    "- n_r: read noise from the electronics\n",
    "\n",
    "CMOS sensors, which emerged in the 1990s, use a different architecture where each pixel has its own amplifier. A key parameter is the fill factor (FF):\n",
    "\n",
    "$FF = \\frac{A_{photosensitive}}{A_{pixel}} × 100\\%$\n",
    "\n",
    "This represents what percentage of each pixel actually captures light, with the rest being taken up by electronic components.\n",
    "\n",
    "## Modern Digital Imaging\n",
    "\n",
    "The conversion of light into digital data involves several steps. First, the continuous light signal must be sampled at discrete points. The Nyquist-Shannon sampling theorem tells us how finely we need to sample to avoid losing information:\n",
    "\n",
    "$f_s > 2f_{max}$\n",
    "\n",
    "The sampling frequency (f_s) must be more than twice the highest frequency (f_max) present in the signal. In imaging terms, this means our pixels must be small enough to capture the finest details we want to resolve.\n",
    "\n",
    "The dynamic range (DR) of a digital imaging system—its ability to capture both very bright and very dark areas—is given by:\n",
    "\n",
    "$DR = 20\\log_{10}(\\frac{FWC}{n_r})$\n",
    "\n",
    "where FWC is the full well capacity (how many electrons each pixel can hold) and n_r is the read noise. The logarithmic scale (in decibels) helps us handle the wide range of values involved.\n",
    "\n",
    "Modern imaging systems convert the continuous world into discrete samples through both spatial sampling and intensity quantization. The spatial resolution is determined by:\n",
    "\n",
    "$p = \\frac{FOV}{N}$\n",
    "\n",
    "where FOV is the field of view and N is the number of pixels. Meanwhile, intensity quantization divides the continuous range of light intensities into discrete levels—typically 256 levels for 8-bit imaging, or 65,536 levels for 16-bit imaging used in scientific applications.\n",
    "\n",
    "Through this journey from the simple camera obscura to modern digital sensors, we see how our understanding of light, chemistry, and electronics has evolved, leading to increasingly sophisticated ways of capturing and representing images. Each advancement built upon previous knowledge, creating the rich field of imaging science we have today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digital Images: Mathematical Foundations <a id='digital'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Digital Images as Discrete Functions\n",
    "\n",
    "A digital image is, at its mathematical core, a discrete function that maps spatial coordinates to intensity values. Let's examine this concept rigorously:\n",
    "\n",
    "### 1.1 From Continuous to Discrete\n",
    "\n",
    "In the physical world, an image is a continuous function $f(x,y)$ where:\n",
    "- $(x,y) \\in \\mathbb{R}^2$ represents continuous spatial coordinates\n",
    "- $f$ maps to a continuous range of intensity values\n",
    "\n",
    "The digitization process transforms this into a discrete function:\n",
    "$f(x,y) \\rightarrow I[m,n]$ where:\n",
    "- $[m,n] \\in \\mathbb{Z}^2$ represents discrete pixel coordinates\n",
    "- $I$ maps to a finite set of intensity values\n",
    "\n",
    "### 1.2 Sampling and Discretization\n",
    "\n",
    "The transformation from continuous to discrete involves two fundamental processes:\n",
    "\n",
    "1. **Spatial Sampling:**\n",
    "   $x = m\\Delta x, y = n\\Delta y$ where:\n",
    "   - $\\Delta x, \\Delta y$ are the sampling intervals\n",
    "   - $m,n$ are integer indices\n",
    "   - The sampling creates a regular grid of points\n",
    "\n",
    "2. **Amplitude Quantization:**\n",
    "   $I[m,n] = Q\\left(\\lfloor\\frac{f(m\\Delta x, n\\Delta y)}{q}\\rfloor\\right)$ where:\n",
    "   - $Q$ is the quantization operator\n",
    "   - $q$ is the quantization step size\n",
    "   - $\\lfloor \\cdot \\rfloor$ denotes the floor function\n",
    "\n",
    "## 2. The Matrix Representation\n",
    "\n",
    "### 2.1 Mathematical Structure\n",
    "\n",
    "A digital image is represented as a matrix $I \\in \\mathbb{R}^{M \\times N}$:\n",
    "\n",
    "$I = \\begin{bmatrix} \n",
    "I[0,0] & I[0,1] & \\cdots & I[0,N-1] \\\\\n",
    "I[1,0] & I[1,1] & \\cdots & I[1,N-1] \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "I[M-1,0] & I[M-1,1] & \\cdots & I[M-1,N-1]\n",
    "\\end{bmatrix}$\n",
    "\n",
    "Key properties:\n",
    "- Each element $I[m,n]$ represents a pixel value\n",
    "- Indices are zero-based by convention\n",
    "- Matrix dimensions determine image resolution\n",
    "\n",
    "### 2.2 Value Domains\n",
    "\n",
    "For an image with bit depth $b$:\n",
    "- Grayscale: $I[m,n] \\in \\{0,1,\\dots,2^b-1\\}$\n",
    "- Common bit depths:\n",
    "  - 8-bit: $I[m,n] \\in \\{0,\\dots,255\\}$\n",
    "  - 12-bit: $I[m,n] \\in \\{0,\\dots,4095\\}$\n",
    "  - 16-bit: $I[m,n] \\in \\{0,\\dots,65535\\}$\n",
    "\n",
    "### 2.3 Matrix Properties\n",
    "\n",
    "1. **Dimensionality:**\n",
    "   - $M$ rows (height)\n",
    "   - $N$ columns (width)\n",
    "   - Total pixels: $M \\times N$\n",
    "\n",
    "2. **Neighborhood Relations:**\n",
    "   For pixel $I[m,n]$:\n",
    "   - 4-connectivity: $\\{I[m±1,n], I[m,n±1]\\}$\n",
    "   - 8-connectivity: $\\{I[m±1,n±1], I[m±1,n], I[m,n±1]\\}$\n",
    "\n",
    "3. **Matrix Indexing:**\n",
    "   - Row-major order: $I[m,n]$\n",
    "   - Column-major order: $I[n,m]$\n",
    "   - Zero-based indexing is standard in most implementations\n",
    "\n",
    "## 3. Color Images and Channel Representation\n",
    "\n",
    "### 3.1 RGB Color Space\n",
    "\n",
    "An RGB image is represented as a 3D tensor $I \\in \\mathbb{R}^{M \\times N \\times 3}$:\n",
    "\n",
    "For each pixel location $[m,n]$:\n",
    "$I[m,n,c] = \\begin{cases}\n",
    "R[m,n] & \\text{if } c = 0 \\\\\n",
    "G[m,n] & \\text{if } c = 1 \\\\\n",
    "B[m,n] & \\text{if } c = 2\n",
    "\\end{cases}$\n",
    "\n",
    "### 3.2 Channel Matrices\n",
    "\n",
    "Each color channel is a separate matrix:\n",
    "- Red channel: $R \\in \\mathbb{R}^{M \\times N}$\n",
    "- Green channel: $G \\in \\mathbb{R}^{M \\times N}$\n",
    "- Blue channel: $B \\in \\mathbb{R}^{M \\times N}$\n",
    "\n",
    "The full image is their combination:\n",
    "$I = \\{R,G,B\\}$\n",
    "\n",
    "### 3.3 Mathematical Properties of Color\n",
    "\n",
    "1. **Color Vector:**\n",
    "   Each pixel is a vector in 3D space:\n",
    "   $\\vec{p}[m,n] = \\begin{bmatrix} R[m,n] \\\\ G[m,n] \\\\ B[m,n] \\end{bmatrix}$\n",
    "\n",
    "2. **Color Space Volume:**\n",
    "   For bit depth $b$:\n",
    "   - Each channel: $[0, 2^b-1]$\n",
    "   - Total possible colors: $(2^b)^3$\n",
    "\n",
    "3. **Intensity Relationships:**\n",
    "   - Maximum intensity: $\\max(R[m,n], G[m,n], B[m,n])$\n",
    "   - Minimum intensity: $\\min(R[m,n], G[m,n], B[m,n])$\n",
    "   - Average intensity: $\\frac{R[m,n] + G[m,n] + B[m,n]}{3}$\n",
    "\n",
    "### 3.4 Grayscale Conversion\n",
    "\n",
    "The standard weighted conversion:\n",
    "$I_{gray}[m,n] = 0.299R[m,n] + 0.587G[m,n] + 0.114B[m,n]$\n",
    "\n",
    "This preserves perceived brightness based on human sensitivity to different wavelengths.\n",
    "\n",
    "## 4. Memory and Storage Considerations\n",
    "\n",
    "### 4.1 Memory Requirements\n",
    "\n",
    "For an $M \\times N$ image with bit depth $b$:\n",
    "- Grayscale: $M \\times N \\times b$ bits\n",
    "- RGB: $M \\times N \\times 3b$ bits\n",
    "\n",
    "### 4.2 Data Types\n",
    "\n",
    "Common numerical representations:\n",
    "1. **Integer Types:**\n",
    "   - uint8: 0 to 255\n",
    "   - uint16: 0 to 65535\n",
    "   - int16: -32768 to 32767\n",
    "\n",
    "2. **Floating Point:**\n",
    "   - float32: ~7 decimal digits precision\n",
    "   - float64: ~15 decimal digits precision\n",
    "\n",
    "The choice of data type affects both precision and memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Image Operations and Transformations <a id='transformations'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Point Operations\n",
    "\n",
    "Point operations modify pixel values independently, where each output pixel depends only on the corresponding input pixel value. These operations can be represented by a transfer function:\n",
    "\n",
    "$g[m,n] = T(f[m,n])$\n",
    "\n",
    "where:\n",
    "- $f[m,n]$ is the input image\n",
    "- $g[m,n]$ is the output image\n",
    "- $T(\\cdot)$ is the transfer function\n",
    "\n",
    "### 1.1 Linear Intensity Transformations\n",
    "\n",
    "The general form of a linear transformation:\n",
    "\n",
    "$g[m,n] = \\alpha f[m,n] + \\beta$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ controls contrast\n",
    "- $\\beta$ controls brightness\n",
    "\n",
    "Properties:\n",
    "- Preserves relative differences between pixels\n",
    "- Can be inverted when $\\alpha \\neq 0$: $f[m,n] = \\frac{g[m,n] - \\beta}{\\alpha}$\n",
    "\n",
    "### 1.2 Gamma Correction\n",
    "\n",
    "The power-law transformation:\n",
    "\n",
    "$g[m,n] = c(f[m,n])^\\gamma$\n",
    "\n",
    "where:\n",
    "- $c$ is a scaling constant\n",
    "- $\\gamma$ is the gamma value\n",
    "- For normalized intensities: $f[m,n], g[m,n] \\in [0,1]$\n",
    "\n",
    "Properties:\n",
    "- $\\gamma < 1$: Enhances dark regions\n",
    "- $\\gamma > 1$: Enhances bright regions\n",
    "- $\\gamma = 1$: Linear transformation\n",
    "\n",
    "### 1.3 Intensity Window/Level\n",
    "\n",
    "For a window width $w$ and center level $l$:\n",
    "\n",
    "$g[m,n] = \\begin{cases} \n",
    "0 & f[m,n] \\leq l - \\frac{w}{2} \\\\\n",
    "\\frac{f[m,n] - (l - \\frac{w}{2})}{w} & l - \\frac{w}{2} < f[m,n] < l + \\frac{w}{2} \\\\\n",
    "1 & f[m,n] \\geq l + \\frac{w}{2}\n",
    "\\end{cases}$\n",
    "\n",
    "## 2. Histogram Analysis\n",
    "\n",
    "### 2.1 Histogram Computation\n",
    "\n",
    "For an image with $L$ possible intensity levels:\n",
    "\n",
    "$h(k) = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} \\delta(f[m,n] - k)$\n",
    "\n",
    "where:\n",
    "- $k \\in \\{0,1,...,L-1\\}$ is the intensity level\n",
    "- $\\delta(x)$ is the Kronecker delta function\n",
    "- $h(k)$ is the number of pixels with intensity $k$\n",
    "\n",
    "The normalized histogram (probability distribution):\n",
    "\n",
    "$p(k) = \\frac{h(k)}{MN}$\n",
    "\n",
    "### 2.2 Histogram Equalization\n",
    "\n",
    "The transformation function:\n",
    "\n",
    "$T(k) = (L-1)\\sum_{i=0}^k p(i) = (L-1)CDF(k)$\n",
    "\n",
    "where $CDF(k)$ is the cumulative distribution function.\n",
    "\n",
    "For continuous case:\n",
    "$s = T(r) = (L-1)\\int_0^r p_r(w)dw$\n",
    "\n",
    "Properties:\n",
    "- Creates approximately uniform distribution\n",
    "- Maximizes entropy\n",
    "- Enhances global contrast\n",
    "\n",
    "## 3. Geometric Transformations\n",
    "\n",
    "### 3.1 Affine Transformations\n",
    "\n",
    "General form in homogeneous coordinates:\n",
    "\n",
    "$\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix} = \n",
    "\\begin{bmatrix} \n",
    "a_{11} & a_{12} & t_x \\\\\n",
    "a_{21} & a_{22} & t_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}$\n",
    "\n",
    "Special cases:\n",
    "\n",
    "1. **Translation:**\n",
    "$\\begin{bmatrix} \n",
    "1 & 0 & t_x \\\\\n",
    "0 & 1 & t_y \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "2. **Rotation by angle θ:**\n",
    "$\\begin{bmatrix} \n",
    "\\cos\\theta & -\\sin\\theta & 0 \\\\\n",
    "\\sin\\theta & \\cos\\theta & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "3. **Scaling:**\n",
    "$\\begin{bmatrix} \n",
    "s_x & 0 & 0 \\\\\n",
    "0 & s_y & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "### 3.2 Interpolation Methods\n",
    "\n",
    "For non-integer coordinates $(x,y)$:\n",
    "\n",
    "1. **Nearest Neighbor:**\n",
    "$g[m,n] = f[\\lfloor x \\rceil, \\lfloor y \\rceil]$\n",
    "\n",
    "2. **Bilinear Interpolation:**\n",
    "Let $\\alpha = x - \\lfloor x \\rfloor$, $\\beta = y - \\lfloor y \\rfloor$\n",
    "\n",
    "$g[m,n] = (1-\\alpha)(1-\\beta)f[\\lfloor x \\rfloor, \\lfloor y \\rfloor] + \\alpha(1-\\beta)f[\\lceil x \\rceil, \\lfloor y \\rfloor] + \\\\\n",
    "(1-\\alpha)\\beta f[\\lfloor x \\rfloor, \\lceil y \\rceil] + \\alpha\\beta f[\\lceil x \\rceil, \\lceil y \\rceil]$\n",
    "\n",
    "3. **Bicubic Interpolation:**\n",
    "Using cubic convolution kernel:\n",
    "$h(x) = \\begin{cases}\n",
    "1 - 2|x|^2 + |x|^3 & 0 \\leq |x| < 1 \\\\\n",
    "4 - 8|x| + 5|x|^2 - |x|^3 & 1 \\leq |x| < 2 \\\\\n",
    "0 & 2 \\leq |x|\n",
    "\\end{cases}$\n",
    "\n",
    "## 4. Fourier Transform Analysis\n",
    "\n",
    "### 4.1 2D Discrete Fourier Transform (DFT)\n",
    "\n",
    "Forward transform:\n",
    "$F[u,v] = \\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} f[m,n]e^{-j2\\pi(\\frac{um}{M} + \\frac{vn}{N})}$\n",
    "\n",
    "Inverse transform:\n",
    "$f[m,n] = \\frac{1}{MN}\\sum_{u=0}^{M-1} \\sum_{v=0}^{N-1} F[u,v]e^{j2\\pi(\\frac{um}{M} + \\frac{vn}{N})}$\n",
    "\n",
    "Properties:\n",
    "1. Linearity: $\\mathcal{F}\\{af_1 + bf_2\\} = a\\mathcal{F}\\{f_1\\} + b\\mathcal{F}\\{f_2\\}$\n",
    "2. Translation: $f[m-m_0,n-n_0] \\leftrightarrow F[u,v]e^{-j2\\pi(\\frac{um_0}{M} + \\frac{vn_0}{N})}$\n",
    "3. Rotation: Rotating image rotates spectrum by same angle\n",
    "4. Scaling: Inverse relationship between spatial and frequency scaling\n",
    "\n",
    "### 4.2 Frequency Domain Analysis\n",
    "\n",
    "Power spectrum:\n",
    "$P[u,v] = |F[u,v]|^2$\n",
    "\n",
    "Phase spectrum:\n",
    "$\\phi[u,v] = \\tan^{-1}\\left(\\frac{\\Im\\{F[u,v]\\}}{\\Re\\{F[u,v]\\}}\\right)$\n",
    "\n",
    "Magnitude spectrum (often displayed in log scale):\n",
    "$D[u,v] = \\log(1 + |F[u,v]|)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Enhancement and Restoration <a id='restoration'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Noise in Digital Images\n",
    "\n",
    "When we capture a digital image, the result is never a perfect representation of the scene. Instead, what we get is the true image contaminated by various forms of noise. To understand this mathematically, we model a noisy image using the equation:\n",
    "\n",
    "$g[m,n] = f[m,n] + \\eta[m,n]$\n",
    "\n",
    "Here, $f[m,n]$ represents the \"true\" image we wanted to capture, $\\eta[m,n]$ is the noise that contaminated our image during acquisition, and $g[m,n]$ is the actual noisy image we obtained. The square brackets [m,n] indicate that we're working with discrete pixel coordinates, where m represents the row and n represents the column in our image matrix.\n",
    "\n",
    "### 1.1 Common Types of Noise and Their Mathematical Models\n",
    "\n",
    "Different imaging conditions and hardware characteristics lead to different types of noise. Understanding these noise models is crucial for choosing the right restoration technique.\n",
    "\n",
    "#### Gaussian (Normal) Noise\n",
    "The most commonly encountered form of noise follows a Gaussian distribution, described by the probability density function:\n",
    "\n",
    "$p(z) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(z-\\mu)^2}{2\\sigma^2}}$\n",
    "\n",
    "This equation might look intimidating, but it's describing something quite straightforward: the probability ($p(z)$) of a pixel being corrupted by a noise value $z$. The parameter μ represents the average noise value (often zero), and σ determines how \"spread out\" the noise is. Gaussian noise typically arises from electronic noise in the sensor or during transmission.\n",
    "\n",
    "#### Poisson (Shot) Noise\n",
    "In low-light imaging or when working with fluorescent samples, we often encounter Poisson noise. Unlike Gaussian noise, Poisson noise is signal-dependent, meaning brighter parts of the image will have more noise. It follows the probability distribution:\n",
    "\n",
    "$p(k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$\n",
    "\n",
    "where λ is both the mean and variance of the distribution. This relationship between mean and variance ($\\sigma^2 = \\lambda$) is a key characteristic that helps us identify and handle Poisson noise.\n",
    "\n",
    "#### Salt-and-Pepper Noise\n",
    "This type of noise manifests as random white and black pixels in the image. It's mathematically described as:\n",
    "\n",
    "$p(z) = \\begin{cases}\n",
    "P_a & \\text{for } z = a \\text{ (pepper, typically 0)} \\\\\n",
    "P_b & \\text{for } z = b \\text{ (salt, typically 255)} \\\\\n",
    "1-P_a-P_b & \\text{for original pixel value}\n",
    "\\end{cases}$\n",
    "\n",
    "This noise often results from transmission errors or faulty pixels in the sensor.\n",
    "\n",
    "### 1.2 Measuring Image Quality: Signal-to-Noise Ratio\n",
    "\n",
    "To quantify how much noise is present in an image, we use the Signal-to-Noise Ratio (SNR):\n",
    "\n",
    "$SNR = 10\\log_{10}\\left(\\frac{\\sigma_f^2}{\\sigma_\\eta^2}\\right)$\n",
    "\n",
    "where $\\sigma_f^2$ is the variance of the true image signal and $\\sigma_\\eta^2$ is the variance of the noise. The logarithmic scale (in decibels) helps us handle the wide range of values we encounter in practice. A higher SNR indicates a cleaner image.\n",
    "\n",
    "## 2. Linear Filtering: The Foundation of Image Enhancement\n",
    "\n",
    "Linear filtering is one of the most fundamental approaches to image enhancement. It works by replacing each pixel with a weighted sum of its neighbors.\n",
    "\n",
    "### 2.1 Understanding Convolution\n",
    "\n",
    "The mathematical operation underlying linear filtering is convolution. For digital images, we use the discrete convolution formula:\n",
    "\n",
    "$g[m,n] = (f * h)[m,n] = \\sum_{k=-a}^a \\sum_{l=-b}^b h[k,l]f[m-k,n-l]$\n",
    "\n",
    "Let's break this down:\n",
    "- $h[k,l]$ is our filter kernel (also called mask or window)\n",
    "- The sums run from -a to a and -b to b, defining the size of our kernel\n",
    "- For each output pixel g[m,n], we:\n",
    "  1. Center the kernel at position [m,n]\n",
    "  2. Multiply each kernel value by the corresponding image pixel\n",
    "  3. Sum all these products to get the filtered pixel value\n",
    "\n",
    "### 2.2 Gaussian Filtering: Smoothing with a Purpose\n",
    "\n",
    "Gaussian filtering is particularly effective for reducing random noise while preserving image structure. The kernel is based on the 2D Gaussian function:\n",
    "\n",
    "$h[k,l] = \\frac{1}{2\\pi\\sigma^2}e^{-\\frac{k^2+l^2}{2\\sigma^2}}$\n",
    "\n",
    "The parameter σ controls the amount of smoothing:\n",
    "- Larger σ values create more blurring\n",
    "- Smaller σ values preserve more detail\n",
    "\n",
    "In practice, we use a discrete approximation:\n",
    "$h[k,l] = Ce^{-\\frac{k^2+l^2}{2\\sigma^2}}$, for $k,l \\in [-a,a]$\n",
    "\n",
    "where C is a normalization constant ensuring the kernel sums to 1:\n",
    "$C = \\frac{1}{\\sum_{k=-a}^a \\sum_{l=-a}^a e^{-\\frac{k^2+l^2}{2\\sigma^2}}}$\n",
    "\n",
    "### 2.3 Mean Filtering: Simple but Effective\n",
    "\n",
    "The mean filter is the simplest form of linear filtering, where all kernel values are equal:\n",
    "\n",
    "$h[k,l] = \\frac{1}{(2a+1)(2b+1)}$\n",
    "\n",
    "This uniform weighting means each pixel is replaced by the average of its neighborhood. While simple, this approach has some important properties:\n",
    "- It preserves the average intensity (DC component) of the image\n",
    "- It reduces random noise effectively\n",
    "- However, it tends to blur edges significantly\n",
    "\n",
    "## 3. Nonlinear Filtering: Beyond Simple Averaging\n",
    "\n",
    "Sometimes linear filtering isn't enough, particularly when we need to preserve edges or handle certain types of noise. This is where nonlinear filters come in.\n",
    "\n",
    "### 3.1 Median Filtering: Robust Noise Removal\n",
    "\n",
    "The median filter works by replacing each pixel with the median value in its neighborhood:\n",
    "\n",
    "$g[m,n] = \\text{median}\\{f[i,j] : [i,j] \\in \\mathcal{N}_{m,n}\\}$\n",
    "\n",
    "Here, $\\mathcal{N}_{m,n}$ represents the neighborhood around pixel [m,n]. The median filter is particularly effective because:\n",
    "- It completely removes salt-and-pepper noise\n",
    "- It preserves edges better than linear filters\n",
    "- It doesn't introduce new pixel values\n",
    "\n",
    "### 3.2 Bilateral Filtering: Edge-Preserving Smoothing\n",
    "\n",
    "The bilateral filter is a sophisticated approach that combines spatial and intensity information:\n",
    "\n",
    "$g[m,n] = \\frac{\\sum_{k,l} f[k,l]w[k,l,m,n]}{\\sum_{k,l} w[k,l,m,n]}$\n",
    "\n",
    "The weight function $w[k,l,m,n]$ has two components:\n",
    "\n",
    "$w[k,l,m,n] = e^{-\\frac{(k-m)^2+(l-n)^2}{2\\sigma_d^2}}e^{-\\frac{(f[k,l]-f[m,n])^2}{2\\sigma_r^2}}$\n",
    "\n",
    "This complex-looking equation actually does something quite intuitive:\n",
    "- The first exponential term reduces the influence of distant pixels\n",
    "- The second exponential term reduces the influence of pixels with very different intensities\n",
    "- $\\sigma_d$ controls the spatial extent of the filter\n",
    "- $\\sigma_r$ controls how much intensity difference is allowed\n",
    "\n",
    "## 4. Image Deconvolution: Recovering the True Image\n",
    "\n",
    "Image deconvolution attempts to reverse the blurring that occurs during image capture. The process starts with the blur model:\n",
    "\n",
    "$g[m,n] = (h * f)[m,n] + \\eta[m,n]$\n",
    "\n",
    "In the frequency domain, this becomes:\n",
    "$G[u,v] = H[u,v]F[u,v] + N[u,v]$\n",
    "\n",
    "where:\n",
    "- $h[m,n]$ is the Point Spread Function (PSF) describing how the imaging system blurs points\n",
    "- $H[u,v]$ is the Optical Transfer Function (the Fourier transform of the PSF)\n",
    "- Capital letters represent Fourier transforms of their lowercase counterparts\n",
    "\n",
    "### 4.1 Wiener Deconvolution\n",
    "\n",
    "Wiener deconvolution provides an optimal solution in the presence of noise:\n",
    "\n",
    "$\\hat{F}[u,v] = \\frac{H^*[u,v]}{|H[u,v]|^2 + K}G[u,v]$\n",
    "\n",
    "This formula:\n",
    "- Uses the complex conjugate $H^*[u,v]$ to reverse the blur\n",
    "- Includes the term K (noise-to-signal power ratio) to prevent noise amplification\n",
    "- Provides the estimated true image spectrum $\\hat{F}[u,v]$\n",
    "\n",
    "### 4.2 Richardson-Lucy Deconvolution\n",
    "\n",
    "For images with Poisson noise (common in microscopy), the Richardson-Lucy algorithm often works better:\n",
    "\n",
    "$f^{(t+1)}[m,n] = f^{(t)}[m,n]\\left(h[-m,-n] * \\frac{g[m,n]}{(h * f^{(t)})[m,n]}\\right)$\n",
    "\n",
    "This iterative approach:\n",
    "- Ensures all pixel values remain positive\n",
    "- Converges to the maximum likelihood solution\n",
    "- Often provides better results than Wiener filtering for low-light images\n",
    "\n",
    "## 5. Frequency Domain Enhancement\n",
    "\n",
    "Working in the frequency domain often provides more control over enhancement operations.\n",
    "\n",
    "### 5.1 Basic Frequency Domain Filters\n",
    "\n",
    "The ideal low-pass filter:\n",
    "$H[u,v] = \\begin{cases}\n",
    "1 & \\text{if } \\sqrt{u^2 + v^2} \\leq D_0 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "And its high-pass counterpart:\n",
    "$H[u,v] = \\begin{cases}\n",
    "0 & \\text{if } \\sqrt{u^2 + v^2} \\leq D_0 \\\\\n",
    "1 & \\text{otherwise}\n",
    "\\end{cases}$\n",
    "\n",
    "Here, $D_0$ is the cutoff frequency, determining which frequencies pass through the filter.\n",
    "\n",
    "### 5.2 Butterworth Filters: Smooth Frequency Response\n",
    "\n",
    "The Butterworth filter provides a smoother transition in frequency response:\n",
    "\n",
    "Low-pass:\n",
    "$H[u,v] = \\frac{1}{1 + [\\sqrt{u^2 + v^2}/D_0]^{2n}}$\n",
    "\n",
    "High-pass:\n",
    "$H[u,v] = \\frac{1}{1 + [D_0/\\sqrt{u^2 + v^2}]^{2n}}$\n",
    "\n",
    "The order n controls how sharp the frequency cutoff is:\n",
    "- Higher n values create sharper transitions\n",
    "- Lower n values provide smoother transitions\n",
    "- This flexibility often makes Butterworth filters more practical than ideal filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
